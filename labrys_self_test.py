#!/usr/bin/env python3
"""
LABRYS Self-Testing System
Recursive validation of LABRYS framework using dual-blade methodology
"""

import os
import sys
import json
import asyncio
import traceback
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict

# Add LABRYS paths
sys.path.append(os.path.join(os.path.dirname(__file__), '.labrys'))
from analytical.self_analysis_engine import SelfAnalysisEngine
from synthesis.self_synthesis_engine import SelfSynthesisEngine
from validation.safety_validator import SafetyValidator
from validation.system_validator import SystemValidator
from coordination.labrys_coordinator import LabrysCoordinator

# Main framework imports
from recursive_labrys_improvement import RecursiveLabrysImprovement
from taskmaster_labrys import TaskMasterLabrys

@dataclass
class TestResult:
    """Result of a single test"""
    test_name: str
    test_category: str
    passed: bool
    duration: float
    details: Dict[str, Any]
    error_message: Optional[str] = None
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

@dataclass
class ValidationSuite:
    """Complete validation suite results"""
    suite_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    test_results: List[TestResult]
    overall_success: bool
    execution_time: float
    success_rate: float
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

class LabrysRecursiveSelfTest:
    """
    Comprehensive self-testing system for LABRYS framework
    Uses the framework itself to validate its own functionality
    """
    
    def __init__(self, labrys_root: str = None):
        self.labrys_root = labrys_root or os.path.join(os.path.dirname(__file__), '.labrys')
        self.test_results = []
        self.validation_suites = []
        
        # Initialize components for testing
        self.analysis_engine = SelfAnalysisEngine(self.labrys_root)
        self.synthesis_engine = SelfSynthesisEngine(self.labrys_root)
        self.safety_validator = SafetyValidator()
        self.system_validator = SystemValidator()
        self.coordinator = LabrysCoordinator()
        self.taskmaster = TaskMasterLabrys()
        self.recursive_improvement = RecursiveLabrysImprovement(self.labrys_root)
        
        # Test configuration
        self.test_timeout = 30  # seconds
        self.max_test_recursion = 3
        self.success_threshold = 0.8  # 80% success rate required
        
    async def execute_recursive_self_test(self) -> Dict[str, Any]:
        """
        Execute comprehensive recursive self-testing
        """
        print("üó≤ LABRYS Recursive Self-Testing System")
        print("   Testing framework using its own methodology")
        print("   " + "="*50)
        
        start_time = datetime.now()
        
        # Test Suite 1: Core Component Testing
        print("\nüìã Test Suite 1: Core Component Validation")
        core_suite = await self._test_core_components()
        self.validation_suites.append(core_suite)
        
        # Test Suite 2: Dual-Blade Integration Testing
        print("\nüìã Test Suite 2: Dual-Blade Integration")
        integration_suite = await self._test_dual_blade_integration()
        self.validation_suites.append(integration_suite)
        
        # Test Suite 3: Recursive Improvement Testing
        print("\nüìã Test Suite 3: Recursive Improvement")
        improvement_suite = await self._test_recursive_improvement()
        self.validation_suites.append(improvement_suite)
        
        # Test Suite 4: Safety and Validation Testing
        print("\nüìã Test Suite 4: Safety and Validation")
        safety_suite = await self._test_safety_validation()
        self.validation_suites.append(safety_suite)
        
        # Test Suite 5: Performance and Convergence Testing
        print("\nüìã Test Suite 5: Performance and Convergence")
        performance_suite = await self._test_performance_convergence()
        self.validation_suites.append(performance_suite)
        
        # Calculate overall results
        total_execution_time = (datetime.now() - start_time).total_seconds()
        overall_results = self._calculate_overall_results(total_execution_time)
        
        # Recursive validation (test the testing system itself)
        print("\nüîÑ Recursive Validation: Testing the Testing System")
        recursive_results = await self._recursive_test_validation()
        
        return {
            "test_execution_summary": overall_results,
            "validation_suites": [asdict(suite) for suite in self.validation_suites],
            "recursive_validation": recursive_results,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _test_core_components(self) -> ValidationSuite:
        """Test core LABRYS components"""
        test_results = []
        
        # Test 1: Analysis Engine
        result = await self._run_test(
            "analysis_engine_initialization",
            "core_components",
            self._test_analysis_engine_init
        )
        test_results.append(result)
        
        # Test 2: Synthesis Engine
        result = await self._run_test(
            "synthesis_engine_initialization",
            "core_components",
            self._test_synthesis_engine_init
        )
        test_results.append(result)
        
        # Test 3: Safety Validator
        result = await self._run_test(
            "safety_validator_initialization",
            "core_components",
            self._test_safety_validator_init
        )
        test_results.append(result)
        
        # Test 4: System Validator
        result = await self._run_test(
            "system_validator_initialization",
            "core_components",
            self._test_system_validator_init
        )
        test_results.append(result)
        
        # Test 5: Coordinator
        result = await self._run_test(
            "coordinator_initialization",
            "core_components",
            self._test_coordinator_init
        )
        test_results.append(result)
        
        # Test 6: TaskMaster
        result = await self._run_test(
            "taskmaster_initialization",
            "core_components",
            self._test_taskmaster_init
        )
        test_results.append(result)
        
        return self._create_validation_suite("Core Components", test_results)
    
    async def _test_dual_blade_integration(self) -> ValidationSuite:
        """Test dual-blade integration"""
        test_results = []
        
        # Test 1: Blade Synchronization
        result = await self._run_test(
            "blade_synchronization",
            "integration",
            self._test_blade_sync
        )
        test_results.append(result)
        
        # Test 2: Analysis-Synthesis Pipeline
        result = await self._run_test(
            "analysis_synthesis_pipeline",
            "integration",
            self._test_analysis_synthesis_pipeline
        )
        test_results.append(result)
        
        # Test 3: Coordinated Workflow
        result = await self._run_test(
            "coordinated_workflow",
            "integration",
            self._test_coordinated_workflow
        )
        test_results.append(result)
        
        return self._create_validation_suite("Dual-Blade Integration", test_results)
    
    async def _test_recursive_improvement(self) -> ValidationSuite:
        """Test recursive improvement functionality"""
        test_results = []
        
        # Test 1: Self-Analysis
        result = await self._run_test(
            "self_analysis",
            "recursive_improvement",
            self._test_self_analysis
        )
        test_results.append(result)
        
        # Test 2: Self-Synthesis
        result = await self._run_test(
            "self_synthesis",
            "recursive_improvement",
            self._test_self_synthesis
        )
        test_results.append(result)
        
        # Test 3: Improvement Loop
        result = await self._run_test(
            "improvement_loop",
            "recursive_improvement",
            self._test_improvement_loop
        )
        test_results.append(result)
        
        # Test 4: Convergence Detection
        result = await self._run_test(
            "convergence_detection",
            "recursive_improvement",
            self._test_convergence_detection
        )
        test_results.append(result)
        
        return self._create_validation_suite("Recursive Improvement", test_results)
    
    async def _test_safety_validation(self) -> ValidationSuite:
        """Test safety and validation systems"""
        test_results = []
        
        # Test 1: Safety Checks
        result = await self._run_test(
            "safety_checks",
            "safety_validation",
            self._test_safety_checks
        )
        test_results.append(result)
        
        # Test 2: Code Validation
        result = await self._run_test(
            "code_validation",
            "safety_validation",
            self._test_code_validation
        )
        test_results.append(result)
        
        # Test 3: Emergency Stop
        result = await self._run_test(
            "emergency_stop",
            "safety_validation",
            self._test_emergency_stop
        )
        test_results.append(result)
        
        return self._create_validation_suite("Safety and Validation", test_results)
    
    async def _test_performance_convergence(self) -> ValidationSuite:
        """Test performance and convergence metrics"""
        test_results = []
        
        # Test 1: Performance Metrics
        result = await self._run_test(
            "performance_metrics",
            "performance",
            self._test_performance_metrics
        )
        test_results.append(result)
        
        # Test 2: Convergence Metrics
        result = await self._run_test(
            "convergence_metrics",
            "performance",
            self._test_convergence_metrics
        )
        test_results.append(result)
        
        return self._create_validation_suite("Performance and Convergence", test_results)
    
    async def _run_test(self, test_name: str, category: str, test_func) -> TestResult:
        """Run a single test with timeout and error handling"""
        start_time = datetime.now()
        
        try:
            print(f"   üîç Running: {test_name}")
            
            # Run test with timeout
            result = await asyncio.wait_for(test_func(), timeout=self.test_timeout)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            test_result = TestResult(
                test_name=test_name,
                test_category=category,
                passed=result.get('passed', False),
                duration=duration,
                details=result.get('details', {}),
                error_message=result.get('error', None)
            )
            
            status = "‚úÖ PASS" if test_result.passed else "‚ùå FAIL"
            print(f"      {status} ({duration:.2f}s)")
            
            return test_result
            
        except asyncio.TimeoutError:
            duration = (datetime.now() - start_time).total_seconds()
            print(f"      ‚è±Ô∏è  TIMEOUT ({duration:.2f}s)")
            
            return TestResult(
                test_name=test_name,
                test_category=category,
                passed=False,
                duration=duration,
                details={},
                error_message="Test timed out"
            )
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            print(f"      üí• ERROR ({duration:.2f}s): {str(e)}")
            
            return TestResult(
                test_name=test_name,
                test_category=category,
                passed=False,
                duration=duration,
                details={"exception": str(e)},
                error_message=str(e)
            )
    
    # Individual test implementations
    async def _test_analysis_engine_init(self) -> Dict[str, Any]:
        """Test analysis engine initialization"""
        try:
            # Test basic initialization
            engine = SelfAnalysisEngine(self.labrys_root)
            
            # Test analysis functionality
            analysis = await engine.analyze_self_architecture()
            
            return {
                'passed': True,
                'details': {
                    'complexity_score': analysis.complexity_score,
                    'maintainability_score': analysis.maintainability_score,
                    'findings_count': len(analysis.findings)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_synthesis_engine_init(self) -> Dict[str, Any]:
        """Test synthesis engine initialization"""
        try:
            engine = SelfSynthesisEngine(self.labrys_root)
            
            # Test basic functionality
            test_suggestions = [{
                'component_target': 'test_component',
                'improvement_type': 'test',
                'description': 'Test improvement',
                'priority': 'low'
            }]
            
            # This will likely fail safely due to missing component
            modifications = await engine.synthesize_improvements(test_suggestions)
            
            return {
                'passed': True,
                'details': {
                    'modifications_attempted': len(modifications),
                    'backup_dir_exists': os.path.exists(engine.backup_dir)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_safety_validator_init(self) -> Dict[str, Any]:
        """Test safety validator initialization"""
        try:
            validator = SafetyValidator()
            
            # Test basic validation
            test_code = 'def test_function():\n    return "Hello, World!"'
            result = await validator.validate_modification(test_code, test_code, "test_component")
            
            return {
                'passed': True,
                'details': {
                    'safety_score': result.overall_safety_score,
                    'approved': result.approved_for_deployment,
                    'checks_count': len(result.safety_checks)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_system_validator_init(self) -> Dict[str, Any]:
        """Test system validator initialization"""
        try:
            validator = SystemValidator()
            
            # Test basic validation
            report = await validator.run_comprehensive_validation()
            
            return {
                'passed': True,
                'details': {
                    'overall_status': report.get('overall_status', 'unknown'),
                    'success_rate': report.get('success_rate', 0),
                    'tests_run': report.get('test_summary', {}).get('total', 0)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_coordinator_init(self) -> Dict[str, Any]:
        """Test coordinator initialization"""
        try:
            coordinator = LabrysCoordinator()
            
            # Test initialization
            init_result = await coordinator.initialize_dual_blades()
            
            return {
                'passed': True,
                'details': {
                    'analytical_status': init_result.get('analytical', {}).get('status', 'unknown'),
                    'synthesis_status': init_result.get('synthesis', {}).get('status', 'unknown'),
                    'synchronization_status': init_result.get('synchronization', {}).get('status', 'unknown')
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_taskmaster_init(self) -> Dict[str, Any]:
        """Test taskmaster initialization"""
        try:
            taskmaster = TaskMasterLabrys()
            
            # Test initialization
            init_result = await taskmaster.initialize_labrys_system()
            
            return {
                'passed': True,
                'details': {
                    'status': init_result.get('status', 'unknown'),
                    'coordination_initialized': taskmaster.coordination_initialized
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_blade_sync(self) -> Dict[str, Any]:
        """Test blade synchronization"""
        try:
            coordinator = LabrysCoordinator()
            init_result = await coordinator.initialize_dual_blades()
            
            sync_status = init_result.get('synchronization', {}).get('status', 'unknown')
            
            return {
                'passed': sync_status == 'synchronized',
                'details': {
                    'sync_status': sync_status,
                    'analytical_response_time': init_result.get('synchronization', {}).get('analytical_response_time', 0),
                    'synthesis_response_time': init_result.get('synchronization', {}).get('synthesis_response_time', 0)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_analysis_synthesis_pipeline(self) -> Dict[str, Any]:
        """Test analysis-synthesis pipeline"""
        try:
            # Test full pipeline
            analysis = await self.analysis_engine.analyze_self_architecture()
            suggestions = await self.analysis_engine.generate_improvement_suggestions(analysis)
            
            return {
                'passed': len(suggestions) > 0,
                'details': {
                    'analysis_findings': len(analysis.findings),
                    'improvement_suggestions': len(suggestions),
                    'complexity_score': analysis.complexity_score
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_coordinated_workflow(self) -> Dict[str, Any]:
        """Test coordinated workflow"""
        try:
            coordinator = LabrysCoordinator()
            await coordinator.initialize_dual_blades()
            
            # Test basic workflow
            workflow_spec = {
                'analytical_tasks': [{'type': 'research', 'query': 'test', 'domain': 'test'}],
                'synthesis_tasks': [{'type': 'code_generation', 'specifications': {'type': 'function', 'name': 'test'}}],
                'dependencies': [],
                'priority': 'medium'
            }
            
            result = await coordinator.execute_coordinated_workflow(workflow_spec)
            
            return {
                'passed': result is not None,
                'details': {
                    'workflow_executed': True,
                    'result_type': type(result).__name__
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_self_analysis(self) -> Dict[str, Any]:
        """Test self-analysis functionality"""
        try:
            analysis = await self.analysis_engine.analyze_self_architecture()
            
            return {
                'passed': analysis.confidence_level > 0.5,
                'details': {
                    'confidence_level': analysis.confidence_level,
                    'complexity_score': analysis.complexity_score,
                    'maintainability_score': analysis.maintainability_score,
                    'component_name': analysis.component_name
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_self_synthesis(self) -> Dict[str, Any]:
        """Test self-synthesis functionality"""
        try:
            # Create minimal test suggestions
            test_suggestions = [
                {
                    'component_target': 'test_component',
                    'improvement_type': 'documentation',
                    'description': 'Add documentation',
                    'priority': 'low'
                }
            ]
            
            modifications = await self.synthesis_engine.synthesize_improvements(test_suggestions)
            
            return {
                'passed': len(modifications) > 0,
                'details': {
                    'modifications_generated': len(modifications),
                    'modifications_attempted': len(modifications)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_improvement_loop(self) -> Dict[str, Any]:
        """Test improvement loop functionality"""
        try:
            # Test single iteration
            result = await self.recursive_improvement.execute_recursive_improvement(max_iterations=1)
            
            return {
                'passed': result.get('status') == 'completed',
                'details': {
                    'iterations_completed': result.get('total_iterations', 0),
                    'status': result.get('status', 'unknown'),
                    'convergence_achieved': result.get('convergence_achieved', False)
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_convergence_detection(self) -> Dict[str, Any]:
        """Test convergence detection"""
        try:
            # Test convergence logic
            test_delta = 0.05  # Below threshold
            test_modifications = []  # No modifications
            
            convergence = self.recursive_improvement._check_convergence(test_delta, test_modifications)
            
            return {
                'passed': convergence,
                'details': {
                    'convergence_detected': convergence,
                    'test_delta': test_delta,
                    'threshold': self.recursive_improvement.convergence_threshold
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_safety_checks(self) -> Dict[str, Any]:
        """Test safety checks"""
        try:
            # Test safe code
            safe_code = 'def safe_function():\n    return "safe"'
            safe_result = await self.safety_validator.validate_modification(safe_code, safe_code, "test")
            
            # Test unsafe code
            unsafe_code = 'import os\nos.system("rm -rf /")'
            unsafe_result = await self.safety_validator.validate_modification(safe_code, unsafe_code, "test")
            
            return {
                'passed': safe_result.approved_for_deployment and not unsafe_result.approved_for_deployment,
                'details': {
                    'safe_code_approved': safe_result.approved_for_deployment,
                    'unsafe_code_rejected': not unsafe_result.approved_for_deployment,
                    'safe_score': safe_result.overall_safety_score,
                    'unsafe_score': unsafe_result.overall_safety_score
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_code_validation(self) -> Dict[str, Any]:
        """Test code validation"""
        try:
            # Test valid code
            valid_code = 'def valid_function():\n    return True'
            valid_result = await self.safety_validator._check_syntax(valid_code)
            
            # Test invalid code
            invalid_code = 'def invalid_function(\n    return True'
            invalid_result = await self.safety_validator._check_syntax(invalid_code)
            
            return {
                'passed': valid_result.passed and not invalid_result.passed,
                'details': {
                    'valid_code_passed': valid_result.passed,
                    'invalid_code_failed': not invalid_result.passed
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_emergency_stop(self) -> Dict[str, Any]:
        """Test emergency stop functionality"""
        try:
            # Test emergency stop mechanism
            try:
                await self.safety_validator.emergency_stop("Test emergency stop")
                return {'passed': False, 'error': 'Emergency stop did not raise exception'}
            except Exception as expected:
                return {
                    'passed': 'EMERGENCY STOP' in str(expected),
                    'details': {
                        'emergency_stop_triggered': True,
                        'exception_message': str(expected)
                    }
                }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_performance_metrics(self) -> Dict[str, Any]:
        """Test performance metrics"""
        try:
            metrics = self.recursive_improvement.performance_metrics
            
            return {
                'passed': isinstance(metrics, dict) and 'total_iterations' in metrics,
                'details': {
                    'metrics_structure': list(metrics.keys()),
                    'metrics_available': len(metrics) > 0
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    async def _test_convergence_metrics(self) -> Dict[str, Any]:
        """Test convergence metrics"""
        try:
            # Test convergence threshold
            threshold = self.recursive_improvement.convergence_threshold
            
            return {
                'passed': 0 < threshold < 1,
                'details': {
                    'convergence_threshold': threshold,
                    'threshold_valid': 0 < threshold < 1
                }
            }
        except Exception as e:
            return {'passed': False, 'error': str(e)}
    
    def _create_validation_suite(self, suite_name: str, test_results: List[TestResult]) -> ValidationSuite:
        """Create a validation suite from test results"""
        passed_tests = len([r for r in test_results if r.passed])
        failed_tests = len([r for r in test_results if not r.passed])
        total_tests = len(test_results)
        
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        overall_success = success_rate >= self.success_threshold
        
        execution_time = sum(r.duration for r in test_results)
        
        return ValidationSuite(
            suite_name=suite_name,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            test_results=test_results,
            overall_success=overall_success,
            execution_time=execution_time,
            success_rate=success_rate
        )
    
    def _calculate_overall_results(self, execution_time: float) -> Dict[str, Any]:
        """Calculate overall test results"""
        total_tests = sum(suite.total_tests for suite in self.validation_suites)
        total_passed = sum(suite.passed_tests for suite in self.validation_suites)
        total_failed = sum(suite.failed_tests for suite in self.validation_suites)
        
        overall_success_rate = total_passed / total_tests if total_tests > 0 else 0
        overall_success = overall_success_rate >= self.success_threshold
        
        return {
            'overall_success': overall_success,
            'overall_success_rate': overall_success_rate,
            'total_tests': total_tests,
            'total_passed': total_passed,
            'total_failed': total_failed,
            'total_execution_time': execution_time,
            'suites_passed': len([s for s in self.validation_suites if s.overall_success]),
            'suites_failed': len([s for s in self.validation_suites if not s.overall_success]),
            'success_threshold': self.success_threshold
        }
    
    async def _recursive_test_validation(self) -> Dict[str, Any]:
        """Recursively validate the testing system itself"""
        try:
            # Test the test system by running a subset recursively
            recursive_depth = 1
            
            # Test that we can instantiate the test system
            recursive_tester = LabrysRecursiveSelfTest(self.labrys_root)
            
            # Test core functionality
            test_result = await recursive_tester._test_analysis_engine_init()
            
            return {
                'recursive_validation_passed': test_result.get('passed', False),
                'recursive_depth': recursive_depth,
                'self_reference_successful': True,
                'details': test_result.get('details', {})
            }
        except Exception as e:
            return {
                'recursive_validation_passed': False,
                'error': str(e),
                'self_reference_successful': False
            }

async def main():
    """Main entry point for LABRYS self-testing"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="LABRYS Recursive Self-Testing System"
    )
    parser.add_argument("--execute", action="store_true", 
                       help="Execute comprehensive self-tests")
    parser.add_argument("--quick", action="store_true",
                       help="Run quick validation tests only")
    parser.add_argument("--report", help="Generate detailed report to file")
    
    args = parser.parse_args()
    
    # Initialize test system
    test_system = LabrysRecursiveSelfTest()
    
    if args.execute or args.quick:
        # Execute tests
        if args.quick:
            print("üöÄ Quick Validation Mode")
            test_system.test_timeout = 10
            test_system.max_test_recursion = 1
        
        results = await test_system.execute_recursive_self_test()
        
        # Display summary
        summary = results['test_execution_summary']
        print(f"\nüèÅ LABRYS Self-Testing Complete!")
        print(f"   Overall Success: {'‚úÖ YES' if summary['overall_success'] else '‚ùå NO'}")
        print(f"   Success Rate: {summary['overall_success_rate']:.1%}")
        print(f"   Tests Passed: {summary['total_passed']}/{summary['total_tests']}")
        print(f"   Execution Time: {summary['total_execution_time']:.2f}s")
        
        # Save results
        if args.report:
            with open(args.report, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"   Detailed report saved to: {args.report}")
        else:
            results_file = "labrys_self_test_results.json"
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"   Results saved to: {results_file}")
    
    else:
        parser.print_help()
        print("\nüó≤ LABRYS Recursive Self-Testing System")
        print("   Test framework using its own methodology")
        print("   Use --execute to run comprehensive tests")

if __name__ == "__main__":
    asyncio.run(main())