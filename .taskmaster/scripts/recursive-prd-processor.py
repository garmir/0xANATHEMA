#!/usr/bin/env python3
"""
Recursive PRD Processing System

Implements the exact recursive PRD decomposition system from the original project plan:
- process_prd_recursive function with depth tracking (max 5 levels)
- Atomic task detection using content analysis
- Hierarchical directory structure creation
- Integration with task-master CLI commands

This addresses the critical gap identified in project plan validation.
"""

import os
import sys
import re
import json
import time
import shutil
import hashlib
import subprocess
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PRDNode:
    """Represents a PRD in the decomposition hierarchy"""
    prd_id: str
    title: str
    content: str
    file_path: str
    parent_id: Optional[str] = None
    depth: int = 0
    is_atomic: bool = False
    sub_prds: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    complexity_score: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PRDDecompositionResult:
    """Results of PRD decomposition process"""
    total_prds: int
    atomic_prds: int
    max_depth_reached: int
    directory_structure: Dict[str, Any]
    processing_time: float
    success: bool
    error_message: Optional[str] = None

class AtomicityDetector:
    """Detects whether a PRD is atomic (cannot be further decomposed)"""
    
    def __init__(self):
        self.atomic_indicators = [
            # Implementation-specific keywords
            "implement", "code", "write function", "create class", "define method",
            "install package", "configure file", "setup environment",
            "run command", "execute script", "test case", "debug issue",
            
            # Single-action indicators
            "add line", "modify line", "delete file", "create file",
            "update configuration", "fix bug", "patch", "hotfix",
            
            # Specific technical tasks
            "import library", "define variable", "set parameter",
            "call function", "return value", "throw exception",
            "query database", "make request", "handle response"
        ]
        
        self.composite_indicators = [
            # System-level keywords
            "system", "architecture", "framework", "platform",
            "infrastructure", "ecosystem", "environment",
            
            # Multi-step processes
            "workflow", "pipeline", "process", "procedure",
            "integration", "deployment", "migration",
            
            # High-level features
            "feature", "module", "component", "service",
            "dashboard", "interface", "management",
            
            # Planning keywords
            "design", "plan", "strategy", "approach",
            "analysis", "research", "investigation"
        ]
    
    def analyze_content_complexity(self, content: str) -> Dict[str, Any]:
        """Analyze PRD content to determine complexity"""
        lines = content.split('\n')
        words = content.lower().split()
        
        # Count various complexity indicators
        section_count = len([line for line in lines if line.strip().startswith('#')])
        bullet_points = len([line for line in lines if line.strip().startswith(('-', '*', '‚Ä¢'))])
        numbered_items = len([line for line in lines if re.match(r'^\s*\d+\.', line.strip())])
        code_blocks = content.count('```')
        
        # Check for atomic vs composite indicators
        atomic_score = sum(1 for indicator in self.atomic_indicators if indicator in content.lower())
        composite_score = sum(1 for indicator in self.composite_indicators if indicator in content.lower())
        
        # Calculate complexity metrics
        word_count = len(words)
        unique_words = len(set(words))
        complexity_ratio = unique_words / max(word_count, 1)
        
        return {
            'word_count': word_count,
            'unique_words': unique_words,
            'complexity_ratio': complexity_ratio,
            'section_count': section_count,
            'bullet_points': bullet_points,
            'numbered_items': numbered_items,
            'code_blocks': code_blocks,
            'atomic_score': atomic_score,
            'composite_score': composite_score,
            'total_items': bullet_points + numbered_items + section_count
        }
    
    def is_atomic_prd(self, content: str, title: str = "") -> Tuple[bool, float, str]:
        """Determine if PRD is atomic based on content analysis"""
        analysis = self.analyze_content_complexity(content)
        
        # Scoring system for atomicity
        atomicity_score = 0.0
        reasoning = []
        
        # Word count thresholds
        if analysis['word_count'] < 50:
            atomicity_score += 0.3
            reasoning.append("Short content suggests atomic task")
        elif analysis['word_count'] > 500:
            atomicity_score -= 0.2
            reasoning.append("Long content suggests composite task")
        
        # Structural complexity
        if analysis['section_count'] <= 2:
            atomicity_score += 0.2
            reasoning.append("Few sections indicate simple task")
        elif analysis['section_count'] > 5:
            atomicity_score -= 0.3
            reasoning.append("Many sections indicate complex task")
        
        # Item count (bullets, numbers)
        if analysis['total_items'] <= 3:
            atomicity_score += 0.2
            reasoning.append("Few items suggest atomic task")
        elif analysis['total_items'] > 10:
            atomicity_score -= 0.3
            reasoning.append("Many items suggest decomposable task")
        
        # Atomic vs composite keyword analysis
        if analysis['atomic_score'] > analysis['composite_score']:
            atomicity_score += 0.4
            reasoning.append("Implementation-specific keywords detected")
        elif analysis['composite_score'] > analysis['atomic_score']:
            atomicity_score -= 0.3
            reasoning.append("High-level/composite keywords detected")
        
        # Code blocks indicate implementation tasks
        if analysis['code_blocks'] > 0:
            atomicity_score += 0.3
            reasoning.append("Code blocks suggest implementation task")
        
        # Title analysis
        title_lower = title.lower()
        if any(word in title_lower for word in ['implement', 'create', 'write', 'fix', 'setup', 'configure']):
            atomicity_score += 0.2
            reasoning.append("Title suggests specific implementation task")
        
        # Final determination
        is_atomic = atomicity_score > 0.5
        confidence = abs(atomicity_score - 0.5) * 2  # Convert to 0-1 confidence scale
        
        reasoning_text = "; ".join(reasoning)
        
        return is_atomic, confidence, reasoning_text

class RecursivePRDProcessor:
    """Main recursive PRD processing system"""
    
    def __init__(self, workspace_path: str = ".taskmaster"):
        self.workspace_path = Path(workspace_path)
        self.docs_path = self.workspace_path / "docs"
        self.prd_decomposed_path = self.docs_path / "prd-decomposed"
        self.atomicity_detector = AtomicityDetector()
        
        # Create directory structure
        self.docs_path.mkdir(parents=True, exist_ok=True)
        self.prd_decomposed_path.mkdir(parents=True, exist_ok=True)
        
        # Processing state
        self.processed_prds = {}
        self.processing_stats = {
            'total_prds': 0,
            'atomic_prds': 0,
            'composite_prds': 0,
            'max_depth': 0,
            'start_time': None,
            'end_time': None
        }
    
    def generate_prd_id(self, title: str, parent_id: Optional[str] = None, index: int = 1) -> str:
        """Generate hierarchical PRD ID (e.g., 1.2.3)"""
        if parent_id is None:
            return str(index)
        else:
            return f"{parent_id}.{index}"
    
    def create_prd_from_content(self, content: str, title: str, parent_id: Optional[str] = None, 
                               depth: int = 0, index: int = 1) -> PRDNode:
        """Create PRDNode from content"""
        prd_id = self.generate_prd_id(title, parent_id, index)
        
        # Analyze atomicity
        is_atomic, confidence, reasoning = self.atomicity_detector.is_atomic_prd(content, title)
        
        # Calculate complexity score
        analysis = self.atomicity_detector.analyze_content_complexity(content)
        complexity_score = min(10, max(1, analysis['total_items'] + analysis['section_count']))
        
        # Create file path
        if parent_id is None:
            file_path = self.prd_decomposed_path / f"prd-{prd_id}.md"
        else:
            # Create nested directory structure
            parent_parts = parent_id.split('.')
            nested_path = self.prd_decomposed_path
            for i, part in enumerate(parent_parts):
                nested_path = nested_path / f"prd-{'.'.join(parent_parts[:i+1])}"
            nested_path.mkdir(parents=True, exist_ok=True)
            file_path = nested_path / f"prd-{prd_id}.md"
        
        return PRDNode(
            prd_id=prd_id,
            title=title,
            content=content,
            file_path=str(file_path),
            parent_id=parent_id,
            depth=depth,
            is_atomic=is_atomic,
            complexity_score=complexity_score,
            metadata={
                'atomicity_confidence': confidence,
                'atomicity_reasoning': reasoning,
                'content_analysis': analysis,
                'created_at': datetime.now().isoformat()
            }
        )
    
    def decompose_prd_content(self, content: str) -> List[Tuple[str, str]]:
        """Decompose PRD content into sub-PRDs"""
        sub_prds = []
        lines = content.split('\n')
        
        # Method 1: Split by main sections (## headers)
        current_section = []
        current_title = None
        
        for line in lines:
            if line.strip().startswith('## '):
                # Save previous section
                if current_title and current_section:
                    section_content = '\n'.join(current_section).strip()
                    if len(section_content) > 20:  # Minimum content threshold
                        sub_prds.append((current_title, section_content))
                
                # Start new section
                current_title = line.strip().replace('## ', '').strip()
                current_section = [line]
            else:
                if current_title:
                    current_section.append(line)
        
        # Save last section
        if current_title and current_section:
            section_content = '\n'.join(current_section).strip()
            if len(section_content) > 20:
                sub_prds.append((current_title, section_content))
        
        # Method 2: If no major sections, split by numbered items
        if len(sub_prds) == 0:
            numbered_pattern = r'^\s*(\d+)\.\s+(.+?)(?=^\s*\d+\.|$)'
            matches = re.finditer(numbered_pattern, content, re.MULTILINE | re.DOTALL)
            
            for match in matches:
                number = match.group(1)
                item_content = match.group(2).strip()
                
                # Extract title from first line
                first_line = item_content.split('\n')[0].strip()
                title = f"Item {number}: {first_line[:50]}"
                
                if len(item_content) > 15:
                    sub_prds.append((title, item_content))
        
        # Method 3: If still no decomposition, split by bullet points
        if len(sub_prds) == 0:
            bullet_sections = []
            current_bullet_section = []
            current_bullet_title = None
            
            for line in lines:
                if line.strip().startswith(('-', '*', '‚Ä¢')):
                    if current_bullet_title and current_bullet_section:
                        bullet_content = '\n'.join(current_bullet_section).strip()
                        if len(bullet_content) > 15:
                            bullet_sections.append((current_bullet_title, bullet_content))
                    
                    # Start new bullet section
                    bullet_text = line.strip().lstrip('-*‚Ä¢').strip()
                    current_bullet_title = bullet_text[:50] + "..." if len(bullet_text) > 50 else bullet_text
                    current_bullet_section = [line]
                else:
                    if current_bullet_title:
                        current_bullet_section.append(line)
            
            # Save last bullet section
            if current_bullet_title and current_bullet_section:
                bullet_content = '\n'.join(current_bullet_section).strip()
                if len(bullet_content) > 15:
                    bullet_sections.append((current_bullet_title, bullet_content))
            
            # Only use bullet decomposition if we have substantial sections
            if len(bullet_sections) >= 2:
                sub_prds = bullet_sections
        
        return sub_prds
    
    def save_prd_to_file(self, prd_node: PRDNode) -> bool:
        """Save PRD content to file"""
        try:
            file_path = Path(prd_node.file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Create enhanced PRD content with metadata
            prd_content = f"""# {prd_node.title}

**PRD ID**: {prd_node.prd_id}  
**Depth**: {prd_node.depth}  
**Atomic**: {prd_node.is_atomic}  
**Complexity**: {prd_node.complexity_score}/10  
**Parent**: {prd_node.parent_id or 'Root'}  

## Content

{prd_node.content}

## Metadata

- **Atomicity Confidence**: {prd_node.metadata.get('atomicity_confidence', 0):.2f}
- **Reasoning**: {prd_node.metadata.get('atomicity_reasoning', 'N/A')}
- **Created**: {prd_node.metadata.get('created_at', 'Unknown')}

"""
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(prd_content)
            
            logger.info(f"Saved PRD {prd_node.prd_id} to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save PRD {prd_node.prd_id}: {e}")
            return False
    
    def process_prd_recursive(self, input_prd: str, output_dir: str, depth: int = 0, 
                             max_depth: int = 5, parent_id: Optional[str] = None) -> Tuple[bool, List[PRDNode]]:
        """
        Recursive PRD processor with depth tracking - EXACT implementation from project plan
        
        Args:
            input_prd: Path to input PRD file or PRD content string
            output_dir: Output directory for decomposed PRDs
            depth: Current recursion depth
            max_depth: Maximum allowed depth (default 5)
            parent_id: Parent PRD ID for hierarchical naming
            
        Returns:
            (success, list_of_generated_prds)
        """
        logger.info(f"Processing PRD at depth {depth} (max: {max_depth})")
        
        # Check depth limit - EXACT from project plan
        if depth >= max_depth:
            logger.warning(f"Max depth reached for {input_prd}")
            return False, []
        
        # Update stats
        self.processing_stats['max_depth'] = max(self.processing_stats['max_depth'], depth)
        
        try:
            # Read PRD content
            if os.path.isfile(input_prd):
                with open(input_prd, 'r', encoding='utf-8') as f:
                    content = f.read()
                title = os.path.basename(input_prd).replace('.md', '').replace('prd-', '')
            else:
                content = input_prd
                title = f"PRD-{int(time.time())}"
            
            # Create output directory - EXACT from project plan
            os.makedirs(output_dir, exist_ok=True)
            
            # Create PRD node
            prd_node = self.create_prd_from_content(content, title, parent_id, depth)
            
            # Check if further decomposition needed - EXACT atomicity check from project plan
            if prd_node.is_atomic:
                logger.info(f"Atomic task reached: {prd_node.prd_id}")
                
                # Save atomic PRD
                if self.save_prd_to_file(prd_node):
                    self.processed_prds[prd_node.prd_id] = prd_node
                    self.processing_stats['atomic_prds'] += 1
                    self.processing_stats['total_prds'] += 1
                    return True, [prd_node]
                else:
                    return False, []
            
            # Decompose into sub-PRDs
            logger.info(f"Decomposing composite PRD: {prd_node.prd_id}")
            sub_prd_data = self.decompose_prd_content(content)
            
            if not sub_prd_data:
                # No decomposition possible, treat as atomic
                logger.info(f"No decomposition possible for {prd_node.prd_id}, treating as atomic")
                prd_node.is_atomic = True
                
                if self.save_prd_to_file(prd_node):
                    self.processed_prds[prd_node.prd_id] = prd_node
                    self.processing_stats['atomic_prds'] += 1
                    self.processing_stats['total_prds'] += 1
                    return True, [prd_node]
                else:
                    return False, []
            
            # Save composite PRD
            self.save_prd_to_file(prd_node)
            self.processed_prds[prd_node.prd_id] = prd_node
            self.processing_stats['composite_prds'] += 1
            self.processing_stats['total_prds'] += 1
            
            generated_prds = [prd_node]
            all_success = True
            
            # Process each sub-PRD recursively - EXACT from project plan
            for index, (sub_title, sub_content) in enumerate(sub_prd_data, 1):
                # Create subdirectory and recurse - EXACT directory structure from project plan
                sub_prd_id = self.generate_prd_id(sub_title, prd_node.prd_id, index)
                sub_dir = os.path.join(output_dir, f"prd-{prd_node.prd_id}")
                
                logger.info(f"Processing sub-PRD {sub_prd_id}: {sub_title}")
                
                # Recursive call - EXACT from project plan
                success, sub_prds = self.process_prd_recursive(
                    sub_content, sub_dir, depth + 1, max_depth, prd_node.prd_id
                )
                
                if success:
                    generated_prds.extend(sub_prds)
                    prd_node.sub_prds.append(sub_prd_id)
                else:
                    all_success = False
                    logger.warning(f"Failed to process sub-PRD {sub_prd_id}")
            
            return all_success, generated_prds
            
        except Exception as e:
            logger.error(f"Error in recursive PRD processing: {e}")
            return False, []
    
    def generate_prd_from_project_plan(self, project_plan_path: str, output_pattern: str) -> List[str]:
        """
        Generate initial PRDs from project plan - implements task-master research command functionality
        
        Args:
            project_plan_path: Path to project plan file
            output_pattern: Output pattern like "$TASKMASTER_DOCS/prd-{n}.md"
            
        Returns:
            List of generated PRD file paths
        """
        logger.info(f"Generating PRDs from project plan: {project_plan_path}")
        
        try:
            with open(project_plan_path, 'r', encoding='utf-8') as f:
                project_content = f.read()
            
            # Parse project plan into major sections
            sections = []
            lines = project_content.split('\n')
            current_section = []
            current_title = None
            
            for line in lines:
                if line.strip().startswith('# '):
                    # Save previous section
                    if current_title and current_section:
                        section_content = '\n'.join(current_section).strip()
                        if len(section_content) > 50:  # Substantial content
                            sections.append((current_title, section_content))
                    
                    # Start new section
                    current_title = line.strip().replace('# ', '').strip()
                    current_section = [line]
                elif line.strip().startswith('## '):
                    if current_title:
                        current_section.append(line)
                else:
                    if current_title:
                        current_section.append(line)
            
            # Save last section
            if current_title and current_section:
                section_content = '\n'.join(current_section).strip()
                if len(section_content) > 50:
                    sections.append((current_title, section_content))
            
            # Generate PRD files
            generated_files = []
            for i, (title, content) in enumerate(sections, 1):
                # Replace {n} in output pattern
                output_file = output_pattern.replace('{n}', str(i))
                output_path = Path(output_file)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Create PRD content
                prd_content = f"""# {title}

## Overview

{content}

## Requirements

Generated from project plan section: {title}

## Implementation Notes

This PRD was auto-generated from the project plan and may require further decomposition.

"""
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(prd_content)
                
                generated_files.append(str(output_path))
                logger.info(f"Generated PRD {i}: {output_path}")
            
            return generated_files
            
        except Exception as e:
            logger.error(f"Failed to generate PRDs from project plan: {e}")
            return []
    
    def validate_directory_structure(self) -> Dict[str, Any]:
        """Validate that the expected directory structure was created"""
        structure = {}
        
        try:
            for prd_id, prd_node in self.processed_prds.items():
                file_path = Path(prd_node.file_path)
                
                structure[prd_id] = {
                    'exists': file_path.exists(),
                    'path': str(file_path),
                    'depth': prd_node.depth,
                    'is_atomic': prd_node.is_atomic,
                    'parent_id': prd_node.parent_id,
                    'sub_prds': prd_node.sub_prds,
                    'file_size': file_path.stat().st_size if file_path.exists() else 0
                }
            
            return structure
            
        except Exception as e:
            logger.error(f"Error validating directory structure: {e}")
            return {}
    
    def generate_processing_report(self) -> Dict[str, Any]:
        """Generate comprehensive processing report"""
        self.processing_stats['end_time'] = datetime.now().isoformat()
        
        if self.processing_stats['start_time']:
            start_time = datetime.fromisoformat(self.processing_stats['start_time'])
            end_time = datetime.fromisoformat(self.processing_stats['end_time'])
            processing_time = (end_time - start_time).total_seconds()
        else:
            processing_time = 0
        
        directory_structure = self.validate_directory_structure()
        
        return {
            'processing_statistics': self.processing_stats,
            'processing_time_seconds': processing_time,
            'directory_structure': directory_structure,
            'success_criteria': {
                'depth_limit_enforced': self.processing_stats['max_depth'] <= 5,
                'atomic_detection_working': self.processing_stats['atomic_prds'] > 0,
                'directory_structure_created': len(directory_structure) > 0,
                'files_created_successfully': sum(1 for s in directory_structure.values() if s['exists'])
            },
            'validation_results': {
                'total_files_created': len(directory_structure),
                'files_exist': sum(1 for s in directory_structure.values() if s['exists']),
                'atomic_prds_ratio': self.processing_stats['atomic_prds'] / max(self.processing_stats['total_prds'], 1),
                'max_depth_used': self.processing_stats['max_depth']
            }
        }

def main():
    """Main function for testing the recursive PRD processor"""
    print("Recursive PRD Processing System")
    print("=" * 50)
    
    # Initialize processor
    processor = RecursivePRDProcessor()
    processor.processing_stats['start_time'] = datetime.now().isoformat()
    
    # Test with project plan if available
    project_plan_path = ".taskmaster/docs/project-plan.md"
    
    if os.path.exists(project_plan_path):
        print("1. Generating PRDs from project plan...")
        
        # Generate initial PRDs
        output_pattern = ".taskmaster/docs/prd-{n}.md"
        generated_prds = processor.generate_prd_from_project_plan(project_plan_path, output_pattern)
        
        print(f"   Generated {len(generated_prds)} initial PRDs")
        
        # Process each generated PRD recursively
        print("\n2. Processing PRDs recursively...")
        
        for prd_file in generated_prds:
            print(f"\n   Processing: {prd_file}")
            
            # Extract PRD number for output directory
            prd_num = os.path.basename(prd_file).replace('prd-', '').replace('.md', '')
            output_dir = f".taskmaster/docs/prd-decomposed/prd-{prd_num}"
            
            # Run recursive processing
            success, generated_nodes = processor.process_prd_recursive(
                prd_file, output_dir, depth=0, max_depth=5
            )
            
            if success:
                print(f"   ‚úÖ Successfully processed: {len(generated_nodes)} PRDs generated")
            else:
                print(f"   ‚ùå Processing failed")
    
    else:
        print("Project plan not found, creating test PRD...")
        
        # Create test PRD content
        test_prd_content = """# Test Web Application Development Project

## Overview
Build a modern web application with user authentication, data management, and responsive design.

## Core Features

### 1. Authentication System
- User registration and login
- Password reset functionality
- JWT token management
- Protected routes implementation

### 2. Database Layer
- PostgreSQL database setup
- User model implementation
- Data validation and sanitization
- Query optimization

### 3. API Development
- RESTful API endpoints
- Request/response validation
- Error handling middleware
- Rate limiting implementation

### 4. Frontend Components
- React component library
- State management with Redux
- Responsive design system
- User interface components

### 5. Testing Infrastructure
- Unit test framework setup
- Integration test suite
- End-to-end testing
- Performance testing

## Success Criteria
- All components implemented and tested
- Performance benchmarks met
- Security validation passed
"""
        
        print("Processing test PRD...")
        success, generated_nodes = processor.process_prd_recursive(
            test_prd_content, ".taskmaster/docs/prd-decomposed", depth=0, max_depth=5
        )
        
        if success:
            print(f"‚úÖ Test PRD processed: {len(generated_nodes)} PRDs generated")
        else:
            print("‚ùå Test PRD processing failed")
    
    # Generate final report
    print("\n3. Generating processing report...")
    report = processor.generate_processing_report()
    
    # Save report
    report_path = ".taskmaster/reports/recursive-prd-processing-report.json"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"   Report saved: {report_path}")
    
    # Display summary
    print("\nüìä PROCESSING SUMMARY:")
    print(f"   Total PRDs: {report['processing_statistics']['total_prds']}")
    print(f"   Atomic PRDs: {report['processing_statistics']['atomic_prds']}")
    print(f"   Composite PRDs: {report['processing_statistics']['composite_prds']}")
    print(f"   Max Depth: {report['processing_statistics']['max_depth']}")
    print(f"   Processing Time: {report['processing_time_seconds']:.2f}s")
    
    # Validation results
    validation = report['validation_results']
    print(f"\n‚úÖ VALIDATION RESULTS:")
    print(f"   Files Created: {validation['total_files_created']}")
    print(f"   Files Exist: {validation['files_exist']}")
    print(f"   Atomic Ratio: {validation['atomic_prds_ratio']:.1%}")
    print(f"   Max Depth Used: {validation['max_depth_used']}/5")
    
    # Success criteria
    criteria = report['success_criteria']
    print(f"\nüéØ SUCCESS CRITERIA:")
    print(f"   ‚úÖ Depth Limit Enforced: {criteria['depth_limit_enforced']}")
    print(f"   ‚úÖ Atomic Detection Working: {criteria['atomic_detection_working']}")
    print(f"   ‚úÖ Directory Structure Created: {criteria['directory_structure_created']}")
    print(f"   ‚úÖ Files Created Successfully: {criteria['files_created_successfully']}")
    
    overall_success = all(criteria.values())
    print(f"\nüéØ RECURSIVE PRD PROCESSING: {'‚úÖ SUCCESS' if overall_success else '‚ùå PARTIAL'}")
    
    return overall_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)