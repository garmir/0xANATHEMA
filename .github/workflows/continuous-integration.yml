name: Continuous Integration & Autonomous Assessment

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main ]
  schedule:
    # Run every 2 hours for frequent assessment
    - cron: '0 */2 * * *'
  workflow_dispatch:
    inputs:
      assessment_type:
        description: 'Type of assessment to run'
        required: false
        default: 'comprehensive'
        type: choice
        options:
        - comprehensive
        - quick
        - performance
        - security

env:
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  # Job 1: Repository Synchronization and Assessment
  sync-and-assess:
    name: 🔄 Repository Sync & Assessment
    runs-on: ubuntu-latest
    outputs:
      changes_detected: ${{ steps.sync.outputs.changes_detected }}
      assessment_score: ${{ steps.assess.outputs.assessment_score }}
      requires_action: ${{ steps.assess.outputs.requires_action }}
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: 🔧 Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: 📦 Install Assessment Dependencies
      run: |
        pip install requests python-dotenv asyncio aiohttp
        npm install -g task-master-ai

    - name: 🔄 Synchronize with GitHub
      id: sync
      run: |
        echo "🔄 Synchronizing with GitHub repository..."
        
        # Fetch latest changes
        git fetch origin
        
        # Check for new commits
        if [ "${{ github.event_name }}" = "schedule" ]; then
          # For scheduled runs, pull latest changes
          BEFORE_SHA=$(git rev-parse HEAD)
          git pull origin master || git pull origin main
          AFTER_SHA=$(git rev-parse HEAD)
          
          if [ "$BEFORE_SHA" != "$AFTER_SHA" ]; then
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "🆕 New changes detected and pulled"
            
            # Log changes
            echo "📋 Recent changes:"
            git log --oneline $BEFORE_SHA..$AFTER_SHA
          else
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "✅ Repository up to date"
          fi
        else
          echo "changes_detected=true" >> $GITHUB_OUTPUT
          echo "📥 Processing push/PR event"
        fi

    - name: 🔍 Run Comprehensive Assessment
      id: assess
      run: |
        echo "🔍 Running comprehensive system assessment..."
        
        ASSESSMENT_TYPE="${{ github.event.inputs.assessment_type || 'comprehensive' }}"
        echo "Assessment type: $ASSESSMENT_TYPE"
        
        # Create dynamic assessment script
        cat > dynamic_assessment.py << 'EOF'
        import json
        import os
        import subprocess
        import datetime
        from pathlib import Path
        
        def run_comprehensive_assessment():
            results = {
                "timestamp": datetime.datetime.now().isoformat(),
                "assessment_type": "$ASSESSMENT_TYPE",
                "scores": {},
                "findings": [],
                "recommendations": [],
                "requires_action": False
            }
            
            # 1. Repository Structure Assessment
            repo_score = 0
            critical_files = [
                "unified_autonomous_system.py",
                "labrys_main.py", 
                ".taskmaster/tasks/tasks.json",
                "README.md"
            ]
            
            for file in critical_files:
                if Path(file).exists():
                    repo_score += 25
                    results["findings"].append(f"✅ {file} present")
                else:
                    results["findings"].append(f"⚠️ {file} missing")
                    results["requires_action"] = True
            
            results["scores"]["repository_structure"] = repo_score
            
            # 2. Task Master Integration Assessment
            task_master_score = 0
            try:
                # Check if task-master CLI works
                result = subprocess.run(['task-master', 'list'], 
                                      capture_output=True, text=True, timeout=30)
                if result.returncode == 0:
                    task_master_score += 50
                    results["findings"].append("✅ Task Master CLI functional")
                    
                    # Check task completion rate
                    if "100%" in result.stdout:
                        task_master_score += 50
                        results["findings"].append("✅ All tasks completed")
                    else:
                        results["findings"].append("📊 Tasks in progress")
                        
                else:
                    results["findings"].append("⚠️ Task Master CLI issues")
                    results["requires_action"] = True
                    
            except Exception as e:
                results["findings"].append(f"❌ Task Master assessment failed: {e}")
                results["requires_action"] = True
            
            results["scores"]["task_master_integration"] = task_master_score
            
            # 3. LABRYS Integration Assessment
            labrys_score = 0
            labrys_files = [
                "labrys_main.py",
                "taskmaster_labrys.py",
                ".labrys"
            ]
            
            for file in labrys_files:
                if Path(file).exists():
                    labrys_score += 33
                    results["findings"].append(f"✅ LABRYS component {file} present")
                else:
                    results["findings"].append(f"⚠️ LABRYS component {file} missing")
            
            results["scores"]["labrys_integration"] = min(labrys_score, 100)
            
            # 4. System Performance Assessment
            performance_score = 70  # Base score
            
            # Check for optimization files
            optimization_files = list(Path('.taskmaster').rglob('*optimization*'))
            if optimization_files:
                performance_score += 30
                results["findings"].append(f"✅ Found {len(optimization_files)} optimization files")
            else:
                results["findings"].append("⚠️ No optimization files found")
            
            results["scores"]["system_performance"] = min(performance_score, 100)
            
            # 5. Integration Quality Assessment
            integration_score = 0
            
            # Check for unified system
            if Path("unified_autonomous_system.py").exists():
                integration_score += 40
                results["findings"].append("✅ Unified autonomous system present")
            
            # Check for GitHub Actions
            if Path(".github/workflows").exists():
                workflows = list(Path(".github/workflows").glob("*.yml"))
                integration_score += min(len(workflows) * 20, 60)
                results["findings"].append(f"✅ Found {len(workflows)} GitHub Actions workflows")
            
            results["scores"]["integration_quality"] = min(integration_score, 100)
            
            # Calculate overall score
            scores = results["scores"]
            overall_score = sum(scores.values()) / len(scores)
            results["overall_score"] = round(overall_score, 1)
            
            # Generate recommendations
            if overall_score < 70:
                results["requires_action"] = True
                results["recommendations"].append("System needs significant improvements")
            elif overall_score < 85:
                results["recommendations"].append("System performing well, minor optimizations needed")
            else:
                results["recommendations"].append("System performing excellently")
            
            # Specific recommendations based on scores
            for component, score in scores.items():
                if score < 70:
                    results["recommendations"].append(f"Improve {component.replace('_', ' ')}")
            
            return results
        
        if __name__ == "__main__":
            assessment = run_comprehensive_assessment()
            print(json.dumps(assessment, indent=2))
            
            # Output for GitHub Actions
            print(f"assessment_score={assessment['overall_score']}")
            print(f"requires_action={str(assessment['requires_action']).lower()}")
            
            # Save detailed results
            with open('assessment_results.json', 'w') as f:
                json.dump(assessment, f, indent=2)
        EOF
        
        # Run assessment
        ASSESSMENT_OUTPUT=$(python dynamic_assessment.py)
        echo "$ASSESSMENT_OUTPUT"
        
        # Extract outputs for GitHub Actions
        SCORE=$(echo "$ASSESSMENT_OUTPUT" | grep "assessment_score=" | cut -d'=' -f2)
        ACTION_REQUIRED=$(echo "$ASSESSMENT_OUTPUT" | grep "requires_action=" | cut -d'=' -f2)
        
        echo "assessment_score=$SCORE" >> $GITHUB_OUTPUT
        echo "requires_action=$ACTION_REQUIRED" >> $GITHUB_OUTPUT

    - name: 📊 Generate Assessment Report
      run: |
        echo "📊 Generating comprehensive assessment report..."
        
        # Create markdown report
        cat > assessment_report.md << EOF
        # 🔍 Continuous Integration Assessment Report
        
        **Timestamp:** $(date)
        **Workflow Run:** ${{ github.run_id }}
        **Assessment Type:** ${{ github.event.inputs.assessment_type || 'comprehensive' }}
        **Overall Score:** ${{ steps.assess.outputs.assessment_score }}%
        
        ## 📋 Assessment Summary
        - **Repository Sync:** ${{ steps.sync.outputs.changes_detected == 'true' && '🆕 Changes detected' || '✅ Up to date' }}
        - **System Health:** ${{ steps.assess.outputs.assessment_score }}%
        - **Action Required:** ${{ steps.assess.outputs.requires_action == 'true' && '⚠️ Yes' || '✅ No' }}
        
        ## 🎯 Key Findings
        
        EOF
        
        # Add detailed findings from assessment
        if [ -f "assessment_results.json" ]; then
          python -c "
        import json
        with open('assessment_results.json') as f:
            data = json.load(f)
        for finding in data.get('findings', []):
            print(f'- {finding}')
        " >> assessment_report.md
        fi
        
        cat >> assessment_report.md << EOF
        
        ## 📈 Performance Scores
        
        EOF
        
        # Add scores
        if [ -f "assessment_results.json" ]; then
          python -c "
        import json
        with open('assessment_results.json') as f:
            data = json.load(f)
        for component, score in data.get('scores', {}).items():
            component_name = component.replace('_', ' ').title()
            print(f'- **{component_name}:** {score}%')
        " >> assessment_report.md
        fi
        
        cat >> assessment_report.md << EOF
        
        ## 💡 Recommendations
        
        EOF
        
        # Add recommendations
        if [ -f "assessment_results.json" ]; then
          python -c "
        import json
        with open('assessment_results.json') as f:
            data = json.load(f)
        for rec in data.get('recommendations', []):
            print(f'- {rec}')
        " >> assessment_report.md
        fi
        
        cat >> assessment_report.md << EOF
        
        ---
        
        **Generated by Continuous Integration & Autonomous Assessment Pipeline**
        **Next Assessment:** Scheduled every 2 hours
        EOF
        
        echo "✅ Assessment report generated"

    - name: 📤 Upload Assessment Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: assessment-results-${{ github.run_number }}
        path: |
          assessment_results.json
          assessment_report.md
        retention-days: 30

  # Job 2: Automated System Improvement
  automated-improvement:
    name: 🚀 Automated System Improvement
    runs-on: ubuntu-latest
    needs: sync-and-assess
    if: needs.sync-and-assess.outputs.requires_action == 'true' || github.event_name == 'schedule'
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4

    - name: 🔧 Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: 📦 Install Improvement Dependencies
      run: |
        pip install requests python-dotenv asyncio aiohttp
        npm install -g task-master-ai

    - name: 🔧 Run Automated Improvements
      run: |
        echo "🔧 Running automated system improvements..."
        
        # Create improvement script
        cat > automated_improvements.py << 'EOF'
        import os
        import json
        import subprocess
        import datetime
        from pathlib import Path
        
        def run_automated_improvements():
            improvements = {
                "timestamp": datetime.datetime.now().isoformat(),
                "improvements_applied": [],
                "actions_taken": [],
                "success_rate": 0
            }
            
            total_improvements = 0
            successful_improvements = 0
            
            # 1. Ensure directory structure
            directories = [
                ".taskmaster/logs",
                ".taskmaster/reports", 
                ".taskmaster/optimization",
                ".taskmaster/assessment"
            ]
            
            for directory in directories:
                if not Path(directory).exists():
                    try:
                        os.makedirs(directory, exist_ok=True)
                        improvements["improvements_applied"].append(f"Created directory: {directory}")
                        successful_improvements += 1
                    except Exception as e:
                        improvements["actions_taken"].append(f"Failed to create {directory}: {e}")
                total_improvements += 1
            
            # 2. Run system health checks
            try:
                if Path("unified_autonomous_system.py").exists():
                    result = subprocess.run(['python', 'unified_autonomous_system.py', '--status'], 
                                          capture_output=True, text=True, timeout=60)
                    if result.returncode == 0:
                        improvements["improvements_applied"].append("Unified system status check passed")
                        successful_improvements += 1
                    else:
                        improvements["actions_taken"].append("Unified system needs attention")
                else:
                    improvements["actions_taken"].append("Unified system file missing")
                total_improvements += 1
            except Exception as e:
                improvements["actions_taken"].append(f"System check failed: {e}")
            
            # 3. Run task master validation
            try:
                result = subprocess.run(['task-master', 'list'], 
                                      capture_output=True, text=True, timeout=30)
                if result.returncode == 0:
                    improvements["improvements_applied"].append("Task Master validation passed")
                    successful_improvements += 1
                else:
                    improvements["actions_taken"].append("Task Master needs configuration")
                total_improvements += 1
            except Exception as e:
                improvements["actions_taken"].append(f"Task Master check failed: {e}")
            
            # 4. Generate improvement recommendations
            if Path(".taskmaster/assessment/project_plan_assessment.py").exists():
                try:
                    result = subprocess.run(['python', '.taskmaster/assessment/project_plan_assessment.py'], 
                                          capture_output=True, text=True, timeout=120)
                    if result.returncode == 0:
                        improvements["improvements_applied"].append("Project assessment completed")
                        successful_improvements += 1
                    else:
                        improvements["actions_taken"].append("Project assessment needs review")
                except Exception as e:
                    improvements["actions_taken"].append(f"Assessment failed: {e}")
                total_improvements += 1
            
            # Calculate success rate
            if total_improvements > 0:
                improvements["success_rate"] = (successful_improvements / total_improvements) * 100
            
            return improvements
        
        if __name__ == "__main__":
            result = run_automated_improvements()
            print(json.dumps(result, indent=2))
            
            # Save results
            with open('improvement_results.json', 'w') as f:
                json.dump(result, f, indent=2)
        EOF
        
        python automated_improvements.py

    - name: 🚀 Execute Autonomous Cycles
      if: github.event_name == 'schedule'
      run: |
        echo "🚀 Executing autonomous system cycles..."
        
        # Run unified autonomous system if available
        if [ -f "unified_autonomous_system.py" ]; then
          echo "🤖 Running unified autonomous cycle..."
          timeout 300 python unified_autonomous_system.py --run-cycle || echo "⏰ Autonomous cycle timed out or completed with warnings"
        fi
        
        # Run LABRYS framework if available
        if [ -f "labrys_main.py" ]; then
          echo "🗲 Running LABRYS validation..."
          timeout 120 python labrys_main.py --validate || echo "⏰ LABRYS validation timed out or completed with warnings"
        fi

    - name: 📤 Upload Improvement Results
      uses: actions/upload-artifact@v4
      with:
        name: improvement-results-${{ github.run_number }}
        path: improvement_results.json
        retention-days: 15

  # Job 3: Performance Monitoring & Alerting
  performance-monitoring:
    name: 📊 Performance Monitoring
    runs-on: ubuntu-latest
    needs: [sync-and-assess, automated-improvement]
    if: always()
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4

    - name: 📊 Collect Performance Metrics
      run: |
        echo "📊 Collecting performance metrics..."
        
        # Get metrics from previous jobs
        ASSESSMENT_SCORE="${{ needs.sync-and-assess.outputs.assessment_score }}"
        CHANGES_DETECTED="${{ needs.sync-and-assess.outputs.changes_detected }}"
        REQUIRES_ACTION="${{ needs.sync-and-assess.outputs.requires_action }}"
        
        # Calculate workflow performance
        WORKFLOW_START=$(date -d "${{ github.event.created_at }}" +%s 2>/dev/null || echo "0")
        CURRENT_TIME=$(date +%s)
        EXECUTION_TIME=$((CURRENT_TIME - WORKFLOW_START))
        
        cat > performance_report.json << EOF
        {
          "performance_metrics": {
            "timestamp": "$(date -Iseconds)",
            "workflow_execution_time_seconds": $EXECUTION_TIME,
            "assessment_score": "$ASSESSMENT_SCORE",
            "changes_detected": "$CHANGES_DETECTED",
            "action_required": "$REQUIRES_ACTION"
          },
          "system_health": {
            "overall_score": "$ASSESSMENT_SCORE",
            "monitoring_active": true,
            "automated_improvements": true,
            "continuous_integration": true
          },
          "github_context": {
            "event_name": "${{ github.event_name }}",
            "ref": "${{ github.ref }}",
            "sha": "${{ github.sha }}",
            "run_id": "${{ github.run_id }}",
            "run_number": "${{ github.run_number }}"
          }
        }
        EOF
        
        echo "📊 Performance metrics collected"
        cat performance_report.json

    - name: 🚨 Performance Alerting
      run: |
        echo "🚨 Checking performance thresholds..."
        
        ASSESSMENT_SCORE="${{ needs.sync-and-assess.outputs.assessment_score }}"
        
        if [ -n "$ASSESSMENT_SCORE" ] && [ "${ASSESSMENT_SCORE%.*}" -lt 70 ]; then
          echo "⚠️ PERFORMANCE ALERT: Assessment score below threshold (${ASSESSMENT_SCORE}%)"
          echo "🔧 Automated improvements may be needed"
        elif [ -n "$ASSESSMENT_SCORE" ] && [ "${ASSESSMENT_SCORE%.*}" -ge 90 ]; then
          echo "🎉 EXCELLENT PERFORMANCE: Assessment score ${ASSESSMENT_SCORE}%"
          echo "✅ System performing at optimal levels"
        else
          echo "📊 NORMAL PERFORMANCE: Assessment score ${ASSESSMENT_SCORE}%"
          echo "✅ System within acceptable parameters"
        fi

    - name: 📤 Upload Performance Report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report-${{ github.run_number }}
        path: performance_report.json
        retention-days: 60

    - name: 📋 Final Summary
      run: |
        echo "📋 CONTINUOUS INTEGRATION SUMMARY"
        echo "=================================="
        echo "Assessment Score: ${{ needs.sync-and-assess.outputs.assessment_score }}%"
        echo "Changes Detected: ${{ needs.sync-and-assess.outputs.changes_detected }}"
        echo "Action Required: ${{ needs.sync-and-assess.outputs.requires_action }}"
        echo "Workflow: ${{ github.run_id }}"
        echo "Trigger: ${{ github.event_name }}"
        echo "=================================="
        echo "✅ Continuous Integration & Autonomous Assessment completed"