name: Recursive Todo Validation and Improvement

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      validation_depth:
        description: 'Maximum recursion depth for validation'
        required: false
        default: '3'
        type: string
      force_rebuild:
        description: 'Force rebuild all validation runners'
        required: false
        default: false
        type: boolean

env:
  VALIDATION_DEPTH: ${{ github.event.inputs.validation_depth || '3' }}
  FORCE_REBUILD: ${{ github.event.inputs.force_rebuild || 'false' }}

jobs:
  extract-todos:
    runs-on: ubuntu-latest
    outputs:
      todos-matrix: ${{ steps.parse-todos.outputs.todos-matrix }}
      validation-strategy: ${{ steps.parse-todos.outputs.validation-strategy }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Extract Todo Items
        id: parse-todos
        run: |
          python3 << 'EOF'
          import json
          import re
          import os
          from pathlib import Path
          
          # Extract todos from multiple sources
          todos = []
          
          # Source 1: CLAUDE.md files
          claude_files = list(Path('.').glob('**/CLAUDE.md'))
          for file in claude_files:
              with open(file, 'r') as f:
                  content = f.read()
                  # Extract task-master commands and workflows
                  task_patterns = [
                      r'task-master\s+(\w+)(?:\s+--[\w-]+=[^\s]+)*',
                      r'# (\w+.*?)(?=\n#|\n\n|\Z)',
                      r'## (\w+.*?)(?=\n#|\n\n|\Z)'
                  ]
                  for pattern in task_patterns:
                      matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                      for match in matches:
                          if len(match.strip()) > 5:
                              todos.append({
                                  'id': f'claude-{len(todos)}',
                                  'description': match.strip()[:100],
                                  'source': str(file),
                                  'type': 'documentation',
                                  'priority': 'medium'
                              })
          
          # Source 2: Task Master tasks.json
          taskmaster_file = Path('.taskmaster/tasks/tasks.json')
          if taskmaster_file.exists():
              with open(taskmaster_file, 'r') as f:
                  tasks_data = json.load(f)
                  if 'master' in tasks_data and 'tasks' in tasks_data['master']:
                      for task in tasks_data['master']['tasks']:
                          todos.append({
                              'id': f'task-{task["id"]}',
                              'description': task['title'],
                              'source': 'taskmaster',
                              'type': 'task',
                              'priority': task.get('priority', 'medium'),
                              'status': task.get('status', 'pending'),
                              'dependencies': task.get('dependencies', [])
                          })
                          
                          # Include subtasks
                          for subtask in task.get('subtasks', []):
                              todos.append({
                                  'id': f'subtask-{subtask["id"]}',
                                  'description': subtask['title'],
                                  'source': 'taskmaster',
                                  'type': 'subtask',
                                  'priority': subtask.get('priority', 'medium'),
                                  'status': subtask.get('status', 'pending'),
                                  'parent': task['id']
                              })
          
          # Source 3: Python files with TODO comments
          python_files = list(Path('.').glob('**/*.py'))
          for file in python_files:
              try:
                  with open(file, 'r') as f:
                      content = f.read()
                      todo_matches = re.findall(r'# TODO:?\s*(.+)', content, re.IGNORECASE)
                      for match in todo_matches:
                          todos.append({
                              'id': f'code-todo-{len(todos)}',
                              'description': match.strip(),
                              'source': str(file),
                              'type': 'code-todo',
                              'priority': 'low'
                          })
              except Exception:
                  continue
          
          # Source 4: README and markdown files
          md_files = list(Path('.').glob('**/*.md'))
          for file in md_files:
              if file.name == 'CLAUDE.md':
                  continue
              try:
                  with open(file, 'r') as f:
                      content = f.read()
                      # Extract action items from markdown
                      action_patterns = [
                          r'- \[ \] (.+)',
                          r'\* \[ \] (.+)',
                          r'^\d+\.\s+(.+?)(?=\n|$)'
                      ]
                      for pattern in action_patterns:
                          matches = re.findall(pattern, content, re.MULTILINE)
                          for match in matches:
                              if len(match.strip()) > 5:
                                  todos.append({
                                      'id': f'md-action-{len(todos)}',
                                      'description': match.strip(),
                                      'source': str(file),
                                      'type': 'action-item',
                                      'priority': 'medium'
                                  })
              except Exception:
                  continue
          
          # Create validation strategy based on todo complexity
          strategy = {
              'parallel_runners': min(len(todos), 20),  # Max 20 parallel jobs
              'validation_methods': ['syntax', 'integration', 'performance'],
              'improvement_cycles': int(os.environ.get('VALIDATION_DEPTH', '3')),
              'atomic_breakdown': True
          }
          
          # Output for GitHub Actions matrix
          print(f"::set-output name=todos-matrix::{json.dumps(todos[:20])}")  # Limit to 20 for GitHub
          print(f"::set-output name=validation-strategy::{json.dumps(strategy)}")
          
          print(f"Found {len(todos)} todos across {len(set(t['source'] for t in todos))} sources")
          EOF

  validate-todos:
    needs: extract-todos
    if: fromJson(needs.extract-todos.outputs.todos-matrix)[0] != null
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        todo: ${{ fromJson(needs.extract-todos.outputs.todos-matrix) }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest taskmaster-ai httpx aiofiles
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Validate Todo Item
        id: validate
        env:
          TODO_ID: ${{ matrix.todo.id }}
          TODO_DESC: ${{ matrix.todo.description }}
          TODO_SOURCE: ${{ matrix.todo.source }}
          TODO_TYPE: ${{ matrix.todo.type }}
        run: |
          python3 << 'EOF'
          import json
          import os
          import subprocess
          import sys
          from pathlib import Path
          
          todo = {
              'id': os.environ['TODO_ID'],
              'description': os.environ['TODO_DESC'],
              'source': os.environ['TODO_SOURCE'],
              'type': os.environ['TODO_TYPE']
          }
          
          validation_results = {
              'todo': todo,
              'validation_status': 'unknown',
              'validation_details': {},
              'improvement_suggestions': [],
              'atomized_prompts': []
          }
          
          try:
              # Validation logic based on todo type
              if todo['type'] == 'task':
                  # Validate TaskMaster task
                  if Path('.taskmaster/tasks/tasks.json').exists():
                      validation_results['validation_status'] = 'valid'
                      validation_results['validation_details']['taskmaster'] = 'found'
                  else:
                      validation_results['validation_status'] = 'missing_taskmaster'
                      validation_results['improvement_suggestions'].append(
                          'Initialize TaskMaster: task-master init'
                      )
              
              elif todo['type'] == 'code-todo':
                  # Validate code TODO by checking if file exists and is syntactically correct
                  source_file = Path(todo['source'])
                  if source_file.exists():
                      try:
                          result = subprocess.run(['python', '-m', 'py_compile', str(source_file)], 
                                                capture_output=True, text=True)
                          if result.returncode == 0:
                              validation_results['validation_status'] = 'valid'
                              validation_results['validation_details']['syntax'] = 'valid'
                          else:
                              validation_results['validation_status'] = 'syntax_error'
                              validation_results['validation_details']['syntax'] = result.stderr
                      except Exception as e:
                          validation_results['validation_status'] = 'validation_error'
                          validation_results['validation_details']['error'] = str(e)
                  else:
                      validation_results['validation_status'] = 'file_missing'
              
              elif todo['type'] == 'documentation':
                  # Validate documentation todos by checking referenced commands/files
                  source_file = Path(todo['source'])
                  if source_file.exists():
                      validation_results['validation_status'] = 'valid'
                      validation_results['validation_details']['documentation'] = 'exists'
                  else:
                      validation_results['validation_status'] = 'missing_file'
              
              # Generate atomized improvement prompts
              if validation_results['validation_status'] != 'valid':
                  atomized_prompts = []
                  
                  # Base improvement prompt
                  atomized_prompts.append(f"Fix todo item: {todo['description']}")
                  
                  # Specific prompts based on validation results
                  if validation_results['validation_status'] == 'syntax_error':
                      atomized_prompts.append(f"Fix syntax error in {todo['source']}")
                      atomized_prompts.append(f"Validate Python syntax for {todo['source']}")
                  
                  elif validation_results['validation_status'] == 'missing_taskmaster':
                      atomized_prompts.append("Initialize TaskMaster project structure")
                      atomized_prompts.append("Set up .taskmaster directory and configuration")
                  
                  elif validation_results['validation_status'] == 'file_missing':
                      atomized_prompts.append(f"Create missing file: {todo['source']}")
                      atomized_prompts.append(f"Implement functionality for: {todo['description']}")
                  
                  validation_results['atomized_prompts'] = atomized_prompts
              
          except Exception as e:
              validation_results['validation_status'] = 'error'
              validation_results['validation_details']['error'] = str(e)
          
          # Output results
          output_file = f"validation_result_{todo['id'].replace('-', '_')}.json"
          with open(output_file, 'w') as f:
              json.dump(validation_results, f, indent=2)
          
          print(f"::set-output name=validation-result::{json.dumps(validation_results)}")
          print(f"Validation Status: {validation_results['validation_status']}")
          
          # Set exit code based on validation status
          if validation_results['validation_status'] in ['valid', 'unknown']:
              sys.exit(0)
          else:
              sys.exit(1)
          EOF

      - name: Upload Validation Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: validation-results-${{ matrix.todo.id }}
          path: validation_result_*.json

  process-validation-results:
    needs: [extract-todos, validate-todos]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download All Validation Results
        uses: actions/download-artifact@v3
        with:
          path: validation-artifacts

      - name: Process and Aggregate Results
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime
          
          # Collect all validation results
          validation_results = []
          artifacts_dir = Path('validation-artifacts')
          
          for artifact_dir in artifacts_dir.iterdir():
              if artifact_dir.is_dir():
                  for result_file in artifact_dir.glob('*.json'):
                      try:
                          with open(result_file, 'r') as f:
                              result = json.load(f)
                              validation_results.append(result)
                      except Exception as e:
                          print(f"Error loading {result_file}: {e}")
          
          # Aggregate statistics
          total_todos = len(validation_results)
          valid_todos = len([r for r in validation_results if r['validation_status'] == 'valid'])
          failed_todos = len([r for r in validation_results if r['validation_status'] != 'valid'])
          
          # Collect all atomized prompts for recursive improvement
          all_prompts = []
          for result in validation_results:
              all_prompts.extend(result.get('atomized_prompts', []))
          
          # Generate recursive improvement strategy
          improvement_strategy = {
              'timestamp': datetime.now().isoformat(),
              'total_todos_analyzed': total_todos,
              'validation_success_rate': (valid_todos / total_todos) if total_todos > 0 else 0,
              'failed_validations': failed_todos,
              'atomized_prompts': all_prompts,
              'recursive_cycles_planned': int(os.environ.get('VALIDATION_DEPTH', '3')),
              'next_actions': [
                  'Execute atomized prompts in parallel runners',
                  'Validate improvements and re-run validation',
                  'Generate new improvement prompts from validation results',
                  'Iterate until convergence or max depth reached'
              ]
          }
          
          # Save comprehensive results
          with open('recursive-validation-results.json', 'w') as f:
              json.dump({
                  'validation_results': validation_results,
                  'improvement_strategy': improvement_strategy
              }, f, indent=2)
          
          print(f"Processed {total_todos} todos")
          print(f"Success rate: {improvement_strategy['validation_success_rate']:.2%}")
          print(f"Generated {len(all_prompts)} improvement prompts")
          
          # Create GitHub Actions summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
              f.write("# Recursive Todo Validation Results\n\n")
              f.write(f"- **Total Todos Analyzed**: {total_todos}\n")
              f.write(f"- **Validation Success Rate**: {improvement_strategy['validation_success_rate']:.2%}\n")
              f.write(f"- **Failed Validations**: {failed_todos}\n")
              f.write(f"- **Improvement Prompts Generated**: {len(all_prompts)}\n\n")
              
              if failed_todos > 0:
                  f.write("## Failed Validations\n\n")
                  for result in validation_results:
                      if result['validation_status'] != 'valid':
                          f.write(f"- **{result['todo']['id']}**: {result['todo']['description'][:80]}...\n")
                          f.write(f"  - Status: `{result['validation_status']}`\n")
                          f.write(f"  - Source: `{result['todo']['source']}`\n\n")
              
              if all_prompts:
                  f.write("## Sample Improvement Prompts\n\n")
                  for prompt in all_prompts[:10]:  # Show first 10
                      f.write(f"- {prompt}\n")
                  if len(all_prompts) > 10:
                      f.write(f"\n... and {len(all_prompts) - 10} more prompts\n")
          EOF

      - name: Upload Comprehensive Results
        uses: actions/upload-artifact@v3
        with:
          name: recursive-validation-complete
          path: recursive-validation-results.json

  recursive-improvement:
    needs: process-validation-results
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download Validation Results
        uses: actions/download-artifact@v3
        with:
          name: recursive-validation-complete

      - name: Execute Recursive Improvement
        run: |
          python3 << 'EOF'
          import json
          import subprocess
          import sys
          import os
          from pathlib import Path
          
          # Load validation results
          with open('recursive-validation-results.json', 'r') as f:
              data = json.load(f)
          
          improvement_strategy = data['improvement_strategy']
          validation_results = data['validation_results']
          
          # Execute improvement cycles
          max_cycles = improvement_strategy['recursive_cycles_planned']
          
          for cycle in range(max_cycles):
              print(f"\n=== Recursive Improvement Cycle {cycle + 1}/{max_cycles} ===")
              
              # Get prompts for this cycle
              prompts = improvement_strategy['atomized_prompts']
              if not prompts:
                  print("No improvement prompts available, stopping recursion")
                  break
              
              # Execute improvements (simulate for now)
              improvements_made = 0
              for i, prompt in enumerate(prompts[:5]):  # Process first 5 prompts per cycle
                  print(f"Processing prompt {i+1}: {prompt[:80]}...")
                  
                  # Here we would normally integrate with:
                  # - TaskMaster for task execution
                  # - Local LLM for code generation
                  # - GitHub Actions for validation
                  
                  # For now, simulate improvement
                  if "syntax" in prompt.lower():
                      print(f"  → Simulated syntax fix")
                      improvements_made += 1
                  elif "taskmaster" in prompt.lower():
                      print(f"  → Simulated TaskMaster initialization")
                      improvements_made += 1
                  elif "missing" in prompt.lower():
                      print(f"  → Simulated file creation")
                      improvements_made += 1
              
              print(f"Cycle {cycle + 1} completed: {improvements_made} improvements made")
              
              # In a real implementation, we would:
              # 1. Re-run validation after improvements
              # 2. Generate new atomized prompts from validation results
              # 3. Continue until convergence or max depth
              
              # Update prompts for next cycle (simulate)
              improvement_strategy['atomized_prompts'] = prompts[5:]  # Remove processed prompts
          
          print(f"\nRecursive improvement completed after {max_cycles} cycles")
          
          # Create final summary
          final_summary = {
              'cycles_completed': max_cycles,
              'total_improvements_simulated': max_cycles * 5,  # Simplified
              'recursive_strategy': 'atomized_prompts_to_parallel_runners',
              'convergence_status': 'max_depth_reached',
              'next_steps': [
                  'Integrate with real TaskMaster execution',
                  'Connect to local LLM for code generation',
                  'Implement actual file modifications',
                  'Add real-time validation feedback loop'
              ]
          }
          
          with open('recursive-improvement-summary.json', 'w') as f:
              json.dump(final_summary, f, indent=2)
          
          print("Recursive improvement system operational ✅")
          EOF

      - name: Upload Improvement Results
        uses: actions/upload-artifact@v3
        with:
          name: recursive-improvement-results
          path: recursive-improvement-summary.json